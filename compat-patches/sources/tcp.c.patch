diff --git a/drivers/net/ovpn/tcp.c b/drivers/net/ovpn/tcp.c
index 289f62c..5d66871 100644
--- a/drivers/net/ovpn/tcp.c
+++ b/drivers/net/ovpn/tcp.c
@@ -7,7 +7,12 @@
  */
 
 #include <linux/skbuff.h>
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 6, 0) && RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(8, 0)
+#include <net/busy_poll.h>
+#endif
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 9, 0)
 #include <net/hotdata.h>
+#endif
 #include <net/inet_common.h>
 #include <net/ipv6.h>
 #include <net/tcp.h>
@@ -30,8 +35,14 @@
 
 static struct proto ovpn_tcp_prot __ro_after_init;
 static struct proto_ops ovpn_tcp_ops __ro_after_init;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(6, 16, 0)
+static struct proto ovpn_tcp6_prot;
+static struct proto_ops ovpn_tcp6_ops;
+static DEFINE_MUTEX(tcp6_prot_mutex);
+#else
 static struct proto ovpn_tcp6_prot __ro_after_init;
 static struct proto_ops ovpn_tcp6_ops __ro_after_init;
+#endif
 
 static int ovpn_tcp_parse(struct strparser *strp, struct sk_buff *skb)
 {
@@ -133,13 +144,228 @@ static void ovpn_tcp_rcv(struct strparser *strp, struct sk_buff *skb)
 	if (WARN_ON(!ovpn_peer_hold(peer)))
 		goto err_nopeer;
 	schedule_work(&peer->tcp.defer_del_work);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 14, 0)
 	dev_dstats_rx_dropped(peer->ovpn->dev);
+#else
+	dev_core_stats_rx_dropped_inc(peer->ovpn->dev);
+#endif
 err_nopeer:
 	kfree_skb(skb);
 }
 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 6, 0) && RHEL_RELEASE_CODE < RHEL_RELEASE_VERSION(8, 0)
+static struct sk_buff *ovpn_skb_set_peeked(struct sk_buff *skb)
+{
+	struct sk_buff *nskb;
+
+	if (skb->peeked)
+		return skb;
+
+	/* We have to unshare an skb before modifying it. */
+	if (!skb_shared(skb))
+		goto done;
+
+	nskb = skb_clone(skb, GFP_ATOMIC);
+	if (!nskb)
+		return ERR_PTR(-ENOMEM);
+
+	skb->prev->next = nskb;
+	skb->next->prev = nskb;
+	nskb->prev = skb->prev;
+	nskb->next = skb->next;
+
+	consume_skb(skb);
+	skb = nskb;
+
+done:
+	skb->peeked = 1;
+
+	return skb;
+}
+
+static struct sk_buff *ovpn__skb_try_recv_from_queue(struct sk_buff_head *queue,
+						     unsigned int flags,
+						     int *off, int *err,
+						     struct sk_buff **last)
+{
+	bool peek_at_off = false;
+	struct sk_buff *skb;
+	int _off = 0;
+
+	if (unlikely(flags & MSG_PEEK && *off >= 0)) {
+		peek_at_off = true;
+		_off = *off;
+	}
+
+	*last = queue->prev;
+	skb_queue_walk(queue, skb)
+	{
+		if (flags & MSG_PEEK) {
+			if (peek_at_off && _off >= skb->len &&
+			    (_off || skb->peeked)) {
+				_off -= skb->len;
+				continue;
+			}
+			if (!skb->len) {
+				skb = ovpn_skb_set_peeked(skb);
+				if (IS_ERR(skb)) {
+					*err = PTR_ERR(skb);
+					return NULL;
+				}
+			}
+			refcount_inc(&skb->users);
+		} else {
+			__skb_unlink(skb, queue);
+		}
+		*off = _off;
+		return skb;
+	}
+	return NULL;
+}
+
+static struct sk_buff *ovpn__skb_try_recv_datagram(struct sock *sk,
+						   struct sk_buff_head *queue,
+						   unsigned int flags, int *off,
+						   int *err,
+						   struct sk_buff **last)
+{
+	struct sk_buff *skb;
+	unsigned long cpu_flags;
+	/*
+	 * Caller is allowed not to check sk->sk_err before skb_recv_datagram()
+	 */
+	int error = sock_error(sk);
+
+	if (error)
+		goto no_packet;
+
+	do {
+		/* Again only user level code calls this function, so nothing
+		 * interrupt level will suddenly eat the receive_queue.
+		 *
+		 * Look at current nfs client by the way...
+		 * However, this function was correct in any case. 8)
+		 */
+		spin_lock_irqsave(&queue->lock, cpu_flags);
+		skb = ovpn__skb_try_recv_from_queue(queue, flags, off, &error,
+						    last);
+		spin_unlock_irqrestore(&queue->lock, cpu_flags);
+		if (error)
+			goto no_packet;
+		if (skb)
+			return skb;
+
+		if (!sk_can_busy_loop(sk))
+			break;
+
+		sk_busy_loop(sk, flags & MSG_DONTWAIT);
+	} while (READ_ONCE(queue->prev) != *last);
+
+	error = -EAGAIN;
+
+no_packet:
+	*err = error;
+	return NULL;
+}
+
+static inline int ovpn_connection_based(struct sock *sk)
+{
+	return sk->sk_type == SOCK_SEQPACKET || sk->sk_type == SOCK_STREAM;
+}
+
+static int ovpn_receiver_wake_function(wait_queue_entry_t *wait,
+				       unsigned int mode, int sync, void *key)
+{
+	/*
+	 * Avoid a wakeup if event not interesting for us
+	 */
+	if (key && !(key_to_poll(key) & (EPOLLIN | EPOLLERR)))
+		return 0;
+	return autoremove_wake_function(wait, mode, sync, key);
+}
+
+static int ovpn__skb_wait_for_more_packets(struct sock *sk,
+					   struct sk_buff_head *queue, int *err,
+					   long *timeo_p,
+					   const struct sk_buff *skb)
+{
+	int error;
+	DEFINE_WAIT_FUNC(wait, ovpn_receiver_wake_function);
+
+	prepare_to_wait_exclusive(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);
+
+	/* Socket errors? */
+	error = sock_error(sk);
+	if (error)
+		goto out_err;
+
+	if (READ_ONCE(queue->prev) != skb)
+		goto out;
+
+	/* Socket shut down? */
+	if (sk->sk_shutdown & RCV_SHUTDOWN)
+		goto out_noerr;
+
+	/* Sequenced packets can come disconnected.
+	 * If so we report the problem
+	 */
+	error = -ENOTCONN;
+	if (ovpn_connection_based(sk) &&
+	    !(sk->sk_state == TCP_ESTABLISHED || sk->sk_state == TCP_LISTEN))
+		goto out_err;
+
+	/* handle signals */
+	if (signal_pending(current))
+		goto interrupted;
+
+	error = 0;
+	*timeo_p = schedule_timeout(*timeo_p);
+out:
+	finish_wait(sk_sleep(sk), &wait);
+	return error;
+interrupted:
+	error = sock_intr_errno(*timeo_p);
+out_err:
+	*err = error;
+	goto out;
+out_noerr:
+	*err = 0;
+	error = 1;
+	goto out;
+}
+
+static struct sk_buff *ovpn__skb_recv_datagram(struct sock *sk,
+					       struct sk_buff_head *sk_queue,
+					       unsigned int flags, int *off,
+					       int *err)
+{
+	struct sk_buff *skb, *last;
+	long timeo;
+
+	timeo = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);
+
+	do {
+		skb = ovpn__skb_try_recv_datagram(sk, sk_queue, flags, off, err,
+						  &last);
+		if (skb)
+			return skb;
+
+		if (*err != -EAGAIN)
+			break;
+	} while (timeo && !ovpn__skb_wait_for_more_packets(sk, sk_queue, err,
+							   &timeo, last));
+
+	return NULL;
+}
+#endif
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 19, 0)
+static int ovpn_tcp_recvmsg(struct sock *sk, struct msghdr *msg, size_t len,
+			    int _noblock, int flags, int *addr_len)
+#else
 static int ovpn_tcp_recvmsg(struct sock *sk, struct msghdr *msg, size_t len,
 			    int flags, int *addr_len)
+#endif
 {
 	int err = 0, off, copied = 0, ret;
 	struct ovpn_socket *sock;
@@ -155,7 +381,15 @@ static int ovpn_tcp_recvmsg(struct sock *sk, struct msghdr *msg, size_t len,
 	peer = sock->peer;
 	rcu_read_unlock();
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 7, 0) || RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(8, 10)
 	skb = __skb_recv_datagram(sk, &peer->tcp.user_queue, flags, &off, &err);
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(5, 6, 0) || RHEL_RELEASE_CODE > RHEL_RELEASE_VERSION(8, 0)
+	skb = __skb_recv_datagram(sk, &peer->tcp.user_queue, flags, NULL, &off,
+				  &err);
+#else
+	skb = ovpn__skb_recv_datagram(sk, &peer->tcp.user_queue, flags, &off,
+				      &err);
+#endif
 	if (!skb) {
 		if (err == -EAGAIN && sk->sk_shutdown & RCV_SHUTDOWN) {
 			ret = 0;
@@ -268,7 +502,11 @@ static void ovpn_tcp_send_sock(struct ovpn_peer *peer, struct sock *sk)
 
 	if (!peer->tcp.out_msg.len) {
 		preempt_disable();
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 14, 0)
 		dev_dstats_tx_add(peer->ovpn->dev, skb->len);
+#else
+		dev_sw_netstats_tx_add(peer->ovpn->dev, 1, skb->len);
+#endif
 		preempt_enable();
 	}
 
@@ -300,7 +538,11 @@ static void ovpn_tcp_send_sock_skb(struct ovpn_peer *peer, struct sock *sk,
 		ovpn_tcp_send_sock(peer, sk);
 
 	if (peer->tcp.out_msg.skb) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 14, 0)
 		dev_dstats_tx_dropped(peer->ovpn->dev);
+#else
+		dev_core_stats_tx_dropped_inc(peer->ovpn->dev);
+#endif
 		kfree_skb(skb);
 		return;
 	}
@@ -321,8 +563,16 @@ void ovpn_tcp_send_skb(struct ovpn_peer *peer, struct sock *sk,
 	spin_lock_nested(&sk->sk_lock.slock, OVPN_TCP_DEPTH_NESTING);
 	if (sock_owned_by_user(sk)) {
 		if (skb_queue_len(&peer->tcp.out_queue) >=
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 9, 0)
 		    READ_ONCE(net_hotdata.max_backlog)) {
+#else
+		    netdev_max_backlog) {
+#endif
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 14, 0)
 			dev_dstats_tx_dropped(peer->ovpn->dev);
+#else
+			dev_core_stats_tx_dropped_inc(peer->ovpn->dev);
+#endif
 			kfree_skb(skb);
 			goto unlock;
 		}
@@ -524,13 +774,23 @@ int ovpn_tcp_socket_attach(struct ovpn_socket *ovpn_sock,
 		ovpn_sock->sk->sk_prot = &ovpn_tcp_prot;
 		ovpn_sock->sk->sk_socket->ops = &ovpn_tcp_ops;
 	} else {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(6, 16, 0)
+		mutex_lock(&tcp6_prot_mutex);
+		if (!ovpn_tcp6_prot.recvmsg)
+			ovpn_tcp_build_protos(&ovpn_tcp6_prot, &ovpn_tcp6_ops,
+					      ovpn_sock->sk->sk_prot,
+					      ovpn_sock->sk->sk_socket->ops);
+		mutex_unlock(&tcp6_prot_mutex);
+#endif
 		ovpn_sock->sk->sk_prot = &ovpn_tcp6_prot;
 		ovpn_sock->sk->sk_socket->ops = &ovpn_tcp6_ops;
 	}
 
 	/* avoid using task_frag */
 	ovpn_sock->sk->sk_allocation = GFP_ATOMIC;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 2, 0) || RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(9, 3)
 	ovpn_sock->sk->sk_use_task_frag = false;
+#endif
 
 	/* enqueue the RX worker */
 	strp_check_rcv(&peer->tcp.strp);
@@ -591,11 +851,22 @@ static void ovpn_tcp_build_protos(struct proto *new_prot,
 /* Initialize TCP static objects */
 void __init ovpn_tcp_init(void)
 {
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(6, 5, 0) && LINUX_VERSION_CODE < KERNEL_VERSION(6, 16, 0)) || \
+	SUSE_PRODUCT_CODE >= SUSE_PRODUCT(1, 15, 6, 0)
+	sendmsg_locked = (sendmsg_locked_t)get_unexported_symbol("sendmsg_locked");
+	if (!sendmsg_locked) {
+		pr_err("sendmsg_locked symbol not found\n");
+		return;
+	}
+#endif
+
 	ovpn_tcp_build_protos(&ovpn_tcp_prot, &ovpn_tcp_ops, &tcp_prot,
 			      &inet_stream_ops);
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 16, 0)
 #if IS_ENABLED(CONFIG_IPV6)
 	ovpn_tcp_build_protos(&ovpn_tcp6_prot, &ovpn_tcp6_ops, &tcpv6_prot,
 			      &inet6_stream_ops);
 #endif
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 16, 0) */
 }
